{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjJn2R-Ft8Gi"
   },
   "source": [
    "For this homework, make sure that you format your notbook nicely and cite all sources in the appropriate sections. Programmatically generate or embed any figures or graphs that you need.\n",
    "\n",
    "Names: __Zhen Guo and Vikram Chowdhary__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lqXxpdFiZcF"
   },
   "source": [
    "## DO THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzwlPkjgt8Gq"
   },
   "source": [
    "Step 1: Train your own word embeddings\n",
    "--------------------------------\n",
    "\n",
    "We chose to use the provided Spooky Author dataset. It contains text from works of fiction written by \"spooky authors\" of the public domain - Edgar Allan poe, HP Lovecraft, and Mary Shelley. The features in this dataset are:\n",
    "- id - a unique identifier for each sentence\n",
    "- text - some text written by one of the authors\n",
    "- author - the author of the sentence (EAP: Edgar Allan Poe, HPL: HP Lovecraft; MWS: Mary Wollstonecraft Shelley)\n",
    "The training portion of this dataset has 19579 texts, and the testing portion has 8392\n",
    "\n",
    "Describe what data set you have chosen to compare and contrast with the your chosen provided dataset. Make sure to describe where it comes from and it's general properties.\n",
    "\n",
    "The dataset we selected was found on Kaggle, and consists of 50,000 IMDB reviews. https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:00.340250Z",
     "start_time": "2020-10-24T03:26:59.570883Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQLg8dGdt8Gr",
    "outputId": "77c6811a-38bb-4df9-dffc-7afc239c7e0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zhenguo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Pre-processing and text-normalization\n",
    "\n",
    "The following pre-processing steps are inspired from https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also pre-processed data so that it begins with < s> tokens (and ends with < /s> tokens). Inspired from answer: https://stackoverflow.com/questions/37605710/tokenize-a-paragraph-into-sentence-and-then-into-words-in-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize text to regular expression\n",
    "# code from https://gist.github.com/yamanahlawat/4443c6e9e65e74829dbb6b47dd81764a\n",
    "\n",
    "replacement_patterns = [\n",
    "  (r'won\\'t', 'will not'),\n",
    "  (r'can\\'t', 'cannot'),\n",
    "  (r'i\\'m', 'i am'),\n",
    "  (r'ain\\'t', 'is not'),\n",
    "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
    "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
    "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
    "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
    "  (r'(\\w+)\\'d', '\\g<1> would')\n",
    "]\n",
    "\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n",
    "\n",
    "def replace(text):\n",
    "    s = text\n",
    "    for (pattern, repl) in patterns:\n",
    "        s = re.sub(pattern, repl, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process the paragram so it is tokenized into sentences, \n",
    "    each sentence start with <s> end withh </s>, words are tokenized and normalized for each sentence.\n",
    "    \"\"\"\n",
    "    sent_text = nltk.sent_tokenize(text) # this gives us a list of sentences\n",
    "    \n",
    "    # now loop over each sentence and tokenize it separately\n",
    "    s = []\n",
    "    for sentence in sent_text:\n",
    "        # regualr expression\n",
    "        sentence = replace(sentence)\n",
    "        # tokenize sentence\n",
    "        tokenized_text = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # lematizing and stemming words\n",
    "        ps = SnowballStemmer(\"english\")\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        new_sent = ['<s>']\n",
    "        for word in tokenized_text:\n",
    "            # now remove punctuation\n",
    "            if not word.isalpha():\n",
    "                continue           \n",
    "            # stemming:\n",
    "            word = ps.stem(word)\n",
    "            # lemmatizing\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            new_sent.append(word)\n",
    "\n",
    "        # add begin and end\n",
    "        new_sent.append('</s>')\n",
    "        s = s + new_sent\n",
    "    return s\n",
    "\n",
    "def process_data(series):\n",
    "    # returns text in this format:\n",
    "    # data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "    # \t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "    # \t\t\t['yet', 'another', 'sentence'],\n",
    "    # \t\t\t['one', 'more', 'sentence'],\n",
    "    # \t\t\t['and', 'the', 'final', 'sentence']]\n",
    "    sentences = []\n",
    "    for _,row in series.items():\n",
    "        sentences.append(process_text(row))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('omw-1.4')\n",
    "\n",
    "# Read the file and prepare the training data \n",
    "# so that it is in the following format\n",
    "spooky_authors_train = pd.read_csv('./spooky-author-identification/train.csv')\n",
    "spooky_authors_test = pd.read_csv('./spooky-author-identification/test.csv')\n",
    "df_imdb = pd.read_csv('IMDB_dataset.csv')\n",
    "\n",
    "given_data_train = process_data(spooky_authors_train['text'])\n",
    "given_data_test = process_data(spooky_authors_test['text'])\n",
    "our_data = process_data(df_imdb['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj0A0mCkt8Gt"
   },
   "source": [
    "### a) Train embeddings on GIVEN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:38.482701Z",
     "start_time": "2020-10-24T04:39:28.044970Z"
    },
    "id": "Od_L53GEt8Gv"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "EMBEDDINGS_SIZE = 200\n",
    "\n",
    "# Train the Word2Vec model from Gensim. \n",
    "# Below are the hyperparameters that are most relevant. \n",
    "# But feel free to explore other \n",
    "# options too:\n",
    "# sg = 1\n",
    "# window = 5\n",
    "# size = EMBEDDINGS_SIZE\n",
    "# min_count = 1\n",
    "# train model on spooky authors training data\n",
    "model = Word2Vec(sentences = given_data_train, \n",
    "                 vector_size = EMBEDDINGS_SIZE,\n",
    "                 sg = 1, \n",
    "                 window = 5,  \n",
    "                 min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:43.448249Z",
     "start_time": "2020-10-24T04:39:43.444835Z"
    },
    "id": "xrt52ahnt8Gw",
    "outputId": "9cc68b79-139f-4996-c736-cb3597e8d16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 14996\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "# THIS DOES NOT WORK?\n",
    "# print('Vocab size {}'.format(len(model.wv.vocab)))\n",
    "# https://stackoverflow.com/questions/35596031/gensim-word2vec-find-number-of-words-in-vocabulary\n",
    "print('Vocab size {}'.format(len(model.wv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:39:48.730304Z",
     "start_time": "2020-10-24T04:39:45.451960Z"
    },
    "id": "UUanXgQLt8Gy"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "model.wv.save_word2vec_format('embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGMUTMcmt8G0"
   },
   "source": [
    "### b) Train embedding on YOUR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sN0KvmUKt8G0"
   },
   "outputs": [],
   "source": [
    "# then do a second data set\n",
    "# given data is roughly 70/30 train/test\n",
    "our_data_train = our_data[:35000]\n",
    "our_data_test = our_data[35000:]\n",
    "our_model = Word2Vec(sentences = our_data_train, \n",
    "                     vector_size = EMBEDDINGS_SIZE,\n",
    "                     sg = 1, \n",
    "                     window = 5,  \n",
    "                     min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eYl9sTbniZcL",
    "outputId": "08c29af9-2445-4241-9f82-1226b1f200ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 56862\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size {}'.format(len(our_model.wv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yW8tTM0xiZcL"
   },
   "outputs": [],
   "source": [
    "# You can save file in txt format, then load later if you wish.\n",
    "# our_model.wv.save_word2vec_format('imdb_embeddings.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjzTVFjt8G1"
   },
   "source": [
    "What text-normalization and pre-processing did you do and why? __Sentence begin and end, regular expression expansion, tokenization, punctuation removing, lemmatizing and stemming__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOFmHpH8t8G2"
   },
   "source": [
    "Step 2: Evaluate the differences between the word embeddings\n",
    "----------------------------\n",
    "\n",
    "(make sure to include graphs, figures, and paragraphs with full sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXjy2-OqgvIf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.wv.index_to_key\n",
    "# get 10 most common and uncommon words/vectors? plot difference between them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqmlOW4biZcM",
    "outputId": "c49da923-a978-4329-9847-84fc12e5b71e"
   },
   "outputs": [],
   "source": [
    "model.wv.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlWydbrgv4P"
   },
   "source": [
    "##Write down your analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tmrTVDqt8G2"
   },
   "source": [
    "Cite your sources:\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix2On6zat8G2"
   },
   "source": [
    "Step 3: Feedforward Neural Language Model\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZsCKQWDt8G2"
   },
   "source": [
    "# a) First, encode  your text into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T21:39:09.625031Z",
     "start_time": "2020-10-26T21:39:09.009109Z"
    },
    "id": "ec0KKYj0t8G3"
   },
   "outputs": [],
   "source": [
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:27:54.373208Z",
     "start_time": "2020-10-24T03:27:54.369835Z"
    },
    "id": "U1PrwlBAt8G5"
   },
   "outputs": [],
   "source": [
    "# The size of the ngram language model you want to train\n",
    "# change as needed for your experiments\n",
    "N_GRAM = 2\n",
    "\n",
    "spooky_tokenizer = Tokenizer()\n",
    "imdb_tokenizer = Tokenizer()\n",
    "\n",
    "# spooky authors data\n",
    "# use out own tokenizer\n",
    "spooky_train_list = given_data_train\n",
    "spooky_tokenizer.fit_on_texts(spooky_train_list)\n",
    "spooky_train_encoded = spooky_tokenizer.texts_to_sequences(spooky_train_list)\n",
    "\n",
    "# # our data\n",
    "imdb_train_list = our_data[:35000]\n",
    "imdb_tokenizer.fit_on_texts(imdb_train_list)\n",
    "imdb_train_encoded = imdb_tokenizer.texts_to_sequences(imdb_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCndArPmt8G5"
   },
   "source": [
    "### b) Next, prepare your sequences from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jG42_9Xt8G6"
   },
   "source": [
    "#### Fixed ngram based sequences "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HsoPVS8ct8G7"
   },
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depending on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "        X,\t\t\t    y\n",
    "    this,    process               however\n",
    "    process, however               afforded\n",
    "    however, afforded\t           me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:28.039381Z",
     "start_time": "2020-10-24T05:21:24.941885Z"
    },
    "id": "B_4YqhKTt8G7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_ngram_training_samples(ngram_len: int, data: list) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the training samples out of it.\n",
    "    Parameters:\n",
    "    up to you, we've put in what we used\n",
    "    but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    # TODO: does this make sense????\n",
    "\n",
    "    combined_text = list(itertools.chain.from_iterable(data))\n",
    "    final_ngrams = []\n",
    "    for idx in range(len(combined_text) - ngram_len + 1):\n",
    "        ngram_list = list(combined_text[idx:idx+ngram_len])\n",
    "        final_ngrams.append(ngram_list)\n",
    "        \n",
    "    return final_ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWL6Czlxt8G8"
   },
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:31.213422Z",
     "start_time": "2020-10-24T05:21:31.061759Z"
    },
    "id": "csweN-d1t8G9"
   },
   "outputs": [],
   "source": [
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]\n",
    "def split_ngrams(ngram_list: list) -> list:\n",
    "    x = [] #those are the context words that we need to get embeddings for\n",
    "    y = []\n",
    "    for ngram in ngram_list:\n",
    "        y.append(ngram[-1])\n",
    "        x.append(ngram[:-1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:21:34.675827Z",
     "start_time": "2020-10-24T05:21:33.315288Z"
    },
    "id": "Vjr6vwP5t8G9"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def read_embeddings(text, embeddings, tokenizer):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    \n",
    "    I updated this function so that it takes a list of words as input,\n",
    "    instead of the raw list of list.\n",
    "    '''\n",
    "    \n",
    "    # you may find generating the following two dicts useful:\n",
    "    # word to embedding : {'the':[0....], ...}\n",
    "    # index to embedding : {1:[0....], ...} \n",
    "    # use your tokenizer's word_index to find the index of\n",
    "    # a given word\n",
    "    word_to_embedding = dict()\n",
    "    index_to_embedding = dict()\n",
    "    tok_w_i = tokenizer.word_index\n",
    "    for word in text:\n",
    "        # since we already pre processed data, we no longer need to transform\n",
    "        if word not in word_to_embedding.keys():\n",
    "            word_to_embedding[word] = embeddings.wv[word]\n",
    "            index_to_embedding[tok_w_i[word]] = embeddings.wv[word]\n",
    "                \n",
    "    return word_to_embedding, index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:24.016237Z",
     "start_time": "2020-10-24T05:22:24.011220Z"
    },
    "id": "H6g9g7p6t8G9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, embeddings, tokenizer) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    generator uses yield instead of return\n",
    "    I don't quit understand what num_sequence_per_batch is...\n",
    "    Also, only have a vague idea on how this function can be used,\n",
    "    I would suggest modifying it after we decide how to impliment leaning\n",
    "    \n",
    "    '''\n",
    "    # IDEA: yield num_sequences_per_batch of X, and the same number of y\n",
    "    # transform y to one hot encodings\n",
    "    # assume X, y are lists of text/words/whatever comes out of split_ngrams\n",
    "    cur_idx = 0\n",
    "    tok_w_i = tokenizer.word_index\n",
    "    while cur_idx <= len(X) - num_sequences_per_batch:\n",
    "        X_temp = X[cur_idx:cur_idx + num_sequences_per_batch]\n",
    "        y_temp = y[cur_idx:cur_idx + num_sequences_per_batch]\n",
    "        \n",
    "        # assuming below version of embeddings, one hot encodings is correct\n",
    "        # otherwise keras.preprocessing has a one_hot function that can be used\n",
    "        X_out = []\n",
    "        y_out = []\n",
    "        \n",
    "        for idx in range(len(X_temp)):\n",
    "            # get embeddings for words in X\n",
    "            w_2_e, i_2_e = read_embeddings(X_temp[idx], embeddings, tokenizer)\n",
    "            X_out += list(w_2_e.values())\n",
    "            \n",
    "            # get one-hot for words in y\n",
    "            word_y = y_temp[idx]\n",
    "            y_vect = [0] * len(tok_w_i)\n",
    "            y_vect[tok_w_i[word_y]] = 1\n",
    "            y_out.append(y_vect)\n",
    "        \n",
    "        cur_idx += num_sequences_per_batch\n",
    "        yield np.array(X_out), np.array(y_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T05:22:55.470133Z",
     "start_time": "2020-10-24T05:22:55.398259Z"
    },
    "id": "vgXSWdlMt8G-"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "spooky_n_gram_temp = generate_ngram_training_samples(2, spooky_train_list)\n",
    "X, y = split_ngrams(spooky_n_gram_temp)\n",
    "# initialize data_generator\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "steps_per_epoch = len(spooky_train_list)//num_sequences_per_batch  # Number of batches per epoch\n",
    "train_generator = data_generator(X, y, num_sequences_per_batch, model, spooky_tokenizer)\n",
    "\n",
    "sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_temp = generate_ngram_training_samples(2, imdb_train_list)\n",
    "X2, y2 = split_ngrams(n_gram_temp)\n",
    "movie_train_generator = data_generator(X2, y2, num_sequences_per_batch, our_model, imdb_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzfweqz1t8G-"
   },
   "source": [
    "### d) Train your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally,  use both your trained language models to generate sentences. Compare these with sentences that could be produced using Shannon’s method with the statistical n-gram language models that you implemented earlier in  the semester. Generate at least 50 sentences of length 20 seeded with your choice of unigrams, appropriate to the pre-processing that you conducted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T03:56:50.919869Z",
     "start_time": "2020-10-24T03:56:50.779792Z"
    },
    "id": "KmgNnQj5t8G_"
   },
   "outputs": [],
   "source": [
    "# code to train a feedforward neural language model \n",
    "# on a set of given word embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "\n",
    "# Define the model architecture using Keras Sequential API\n",
    "spooky_nn_model = Sequential()\n",
    "\n",
    "spooky_nn_model.add(Dense(256, input_shape=(200,), activation='sigmoid'))\n",
    "\n",
    "spooky_nn_model.add(Dense(128, activation='sigmoid'))\n",
    "\n",
    "spooky_nn_model.add(Dense(14996, activation='softmax'))\n",
    "\n",
    "spooky_nn_model.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:01:50.757170Z",
     "start_time": "2020-10-24T03:56:53.620836Z"
    },
    "id": "-dWf2qO3t8G_",
    "outputId": "353a73b8-d147-4b8e-f23b-1581cd674266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 16s 106ms/step - loss: 9.3138 - accuracy: 0.0538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f81ef7afc70>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model\n",
    "spooky_nn_model.fit(x=train_generator, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture using Keras Sequential API\n",
    "movie_nn_model = Sequential()\n",
    "\n",
    "movie_nn_model.add(Dense(256, input_shape=(200,), activation='sigmoid'))\n",
    "\n",
    "movie_nn_model.add(Dense(128, activation='sigmoid'))\n",
    "\n",
    "movie_nn_model.add(Dense(len(our_model.wv), activation='softmax'))\n",
    "\n",
    "movie_nn_model.compile(loss='categorical_crossentropy', optimizer='sgd',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 60s 394ms/step - loss: 10.6390 - accuracy: 0.0525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f82210b9610>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training the model\n",
    "movie_nn_model.fit(x=movie_train_generator, \n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QwRhKYwt8HA"
   },
   "source": [
    "### e) Generate Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:13:54.425934Z",
     "start_time": "2020-10-24T04:13:54.418616Z"
    },
    "id": "ewR5ueOJt8HB"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from the model\n",
    "def generate_seq(model: Sequential, \n",
    "                 embeddings,\n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list, \n",
    "                 n_words: int):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "        n_words: generate a sentence of length n_words\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    w_2_e, i_2_e = read_embeddings(seed, embeddings, tokenizer)\n",
    "    X = list(w_2_e.values())\n",
    "    sentence = [\"<s>\"] + X\n",
    "    for i in range(n_words):\n",
    "        y = model.predict(X)\n",
    "        print(y)\n",
    "        sentence.append(y)\n",
    "        X = y\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(spooky_nn_model, \n",
    "                 model,\n",
    "                 spooky_tokenizer, \n",
    "                 ['this'], \n",
    "                 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T04:14:13.123529Z",
     "start_time": "2020-10-24T04:14:13.000264Z"
    },
    "id": "XZ9fShSyt8HB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 200) for input KerasTensor(type_spec=TensorSpec(shape=(None, 200), dtype=tf.float32, name='dense_21_input'), name='dense_21_input', description=\"created by layer 'dense_21_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_7\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_21\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_7\" \"                 f\"(type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=float32)',)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/g__7jx_16dj5dq8w8kblyp380000gr/T/ipykernel_1423/3416403073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m generate_seq(movie_nn_model, \n\u001b[0m\u001b[1;32m      2\u001b[0m                  \u001b[0mour_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0mimdb_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  20)\n",
      "\u001b[0;32m/var/folders/tp/g__7jx_16dj5dq8w8kblyp380000gr/T/ipykernel_1423/165883616.py\u001b[0m in \u001b[0;36mgenerate_seq\u001b[0;34m(model, embeddings, tokenizer, seed, n_words)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"<s>\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/anaconda3/envs/class/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_7\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense_21\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential_7\" \"                 f\"(type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=float32)',)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "generate_seq(movie_nn_model, \n",
    "                 our_model,\n",
    "                 imdb_tokenizer, \n",
    "                 ['this'], \n",
    "                 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w68JVS2jt8HB"
   },
   "source": [
    "### f) Compare your generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE4dcQdut8HC"
   },
   "outputs": [],
   "source": [
    "def perplexity(self, test_sequence):\n",
    "    \"\"\"\n",
    "        Measures the perplexity for the given test sequence with this trained model.\n",
    "        As described in the text, you may assume that this sequence may consist of many sentences \"glued together\".\n",
    "\n",
    "    Parameters:\n",
    "      test_sequence (string): a sequence of space-separated tokens to measure the perplexity of\n",
    "    Returns:\n",
    "      float: the perplexity of the given sequence\n",
    "    \"\"\"\n",
    "    toks = word_tokenize(test_sequence)\n",
    "    s = self.score(test_sequence)\n",
    "    return s**(-1/len(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yet5p8N1t8HC"
   },
   "source": [
    "Sources Cited\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sPu_1h2t8HC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
